{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ner_3_2021.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4CyQGFg4haX"
      },
      "source": [
        "Flair"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRxrYeEV4gVL",
        "outputId": "3cb50563-4535-47e6-83d1-c399294f908a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6Toer0O450W"
      },
      "source": [
        "import os"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fH08d9VJ5Ife"
      },
      "source": [
        "os.chdir( \"/content/gdrive/MyDrive/flair\" ) "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ug6zYGQ85KbW"
      },
      "source": [
        "pip install flair"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdwhJEXL54Aw"
      },
      "source": [
        "from flair.data import Corpus\n",
        "from flair.datasets import WIKINER_ENGLISH"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXGeR8FJ8Bsj"
      },
      "source": [
        "Next, we create __wikiner_corpus__, an instance of the class __Corpus__.<br>\n",
        "Read [here](https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_6_CORPUS.md) the documentation of __Corpus__.<br>\n",
        "__Question 1__: explain, what the __WIKINER__ corpus is.<br>\n",
        "\n",
        "The Wikiner dataset is a NER dataset automatically generated from Wikipedia. We have different entities like S-PER, E_PER, so we can directly see that like previously with our resumees that we not only have one entity for PERSON, but multiple. Also different types of Organisations. \n",
        "\n",
        "<br>\n",
        "\n",
        "Then, we create __tag_dictionary__ which is an __BILUO__-__NER__-encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hqOdEn7565N",
        "outputId": "88ff8276-5c29-4e0c-e683-74db5d243aba"
      },
      "source": [
        "# 1. get the corpus\n",
        "wikiner_corpus: Corpus = WIKINER_ENGLISH().downsample(0.1)\n",
        "print(wikiner_corpus)\n",
        "# 3. make the tag dictionary from the corpus\n",
        "tag_dictionary = Corpus.make_tag_dictionary( wikiner_corpus, tag_type='ner')\n",
        "print(tag_dictionary.idx2item)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-22 10:59:11,386 https://raw.githubusercontent.com/dice-group/FOX/master/input/Wikiner/aij-wikiner-en-wp3.bz2 not found in cache, downloading to /tmp/tmpx7tfxsn0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 6208404/6208404 [00:00<00:00, 13216516.29B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-22 10:59:11,955 copying /tmp/tmpx7tfxsn0 to cache at /root/.flair/datasets/wikiner_english/aij-wikiner-en-wp3.bz2\n",
            "2021-05-22 10:59:11,972 removing temp file /tmp/tmpx7tfxsn0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-22 10:59:20,590 Reading data from /root/.flair/datasets/wikiner_english\n",
            "2021-05-22 10:59:20,594 Train: /root/.flair/datasets/wikiner_english/aij-wikiner-en-wp3.train\n",
            "2021-05-22 10:59:20,597 Dev: None\n",
            "2021-05-22 10:59:20,599 Test: None\n",
            "Corpus: 11514 train + 1279 dev + 1422 test sentences\n",
            "[b'<unk>', b'O', b'B-PER', b'E-PER', b'S-PER', b'S-ORG', b'B-MISC', b'I-MISC', b'E-MISC', b'S-MISC', b'B-LOC', b'E-LOC', b'S-LOC', b'B-ORG', b'I-ORG', b'E-ORG', b'I-LOC', b'I-PER', b'<START>', b'<STOP>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2W4UKrp-7uw8",
        "outputId": "29d63321-1025-451c-f0a5-90a5e208d61b"
      },
      "source": [
        "print(wikiner_corpus.train[73])\n",
        "print(wikiner_corpus.train[73].to_tagged_string())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence: \"In March 2006 , Russia agreed to erase $ 4.74 billion of Algeria 's Soviet-era debt during a visit by President Vladimir Putin to the country , the first by a Russian leader in half a century .\"   [− Tokens: 38  − Token-Labels: \"In <IN> March <NNP> 2006 <CD> , <,> Russia <NNP/S-LOC> agreed <VBD> to <TO> erase <VB> $ <$> 4.74 <CD> billion <CD> of <IN> Algeria <NNP/S-LOC> 's <POS> Soviet-era <JJ/S-MISC> debt <NN> during <IN> a <DT> visit <NN> by <IN> President <NNP> Vladimir <NNP/B-PER> Putin <NNP/E-PER> to <TO> the <DT> country <NN> , <,> the <DT> first <JJ> by <IN> a <DT> Russian <JJ/S-MISC> leader <NN> in <IN> half <PDT> a <DT> century <NN> . <.>\"]\n",
            "In <IN> March <NNP> 2006 <CD> , <,> Russia <NNP/S-LOC> agreed <VBD> to <TO> erase <VB> $ <$> 4.74 <CD> billion <CD> of <IN> Algeria <NNP/S-LOC> 's <POS> Soviet-era <JJ/S-MISC> debt <NN> during <IN> a <DT> visit <NN> by <IN> President <NNP> Vladimir <NNP/B-PER> Putin <NNP/E-PER> to <TO> the <DT> country <NN> , <,> the <DT> first <JJ> by <IN> a <DT> Russian <JJ/S-MISC> leader <NN> in <IN> half <PDT> a <DT> century <NN> . <.>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br-nRr9Q8g1T"
      },
      "source": [
        "Ok, above, we loaded a corpus, a collection of texts, and with this collection the annotation of these texts.<br>\n",
        "Next, we load the data, we prepared using Spacy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LD2GaEJN76XT",
        "outputId": "03b7d938-93cf-4d9b-a720-be2ffed5635c"
      },
      "source": [
        "from flair.data_fetcher import NLPTaskDataFetcher\n",
        "\n",
        "downsample = 1.0 # 1.0 is full data, try a much smaller number like 0.01 to test run the code\n",
        "data_folder = os.getcwd()\n",
        "columns = {0: 'text', 1: 'ner'}\n",
        "\n",
        "# 1. get the corpus\n",
        "corpus: Corpus = NLPTaskDataFetcher.load_column_corpus(data_folder, columns,\n",
        "                                                             train_file='training_data.csv',\n",
        "                                                             test_file='test_data.csv',\n",
        "                                                           dev_file=None).downsample(downsample)\n",
        "print(corpus)\n",
        "\n",
        "# 3. make the tag dictionary from the corpus\n",
        "tag_dictionary = corpus.make_tag_dictionary(tag_type='ner')\n",
        "print(tag_dictionary.idx2item)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-22 10:59:33,427 Reading data from /content/gdrive/My Drive/flair\n",
            "2021-05-22 10:59:33,429 Train: /content/gdrive/My Drive/flair/training_data.csv\n",
            "2021-05-22 10:59:33,431 Dev: None\n",
            "2021-05-22 10:59:33,433 Test: /content/gdrive/My Drive/flair/test_data.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated function (or staticmethod) load_column_corpus. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Corpus: 6290 train + 699 dev + 3322 test sentences\n",
            "[b'<unk>', b'O', b'B-Skills', b'I-Skills', b'L-Skills', b'U-Companies', b'B-Companies', b'I-Companies', b'L-Companies', b'B-Degree', b'I-Degree', b'L-Degree', b'U-Skills', b'U-Degree', b'<START>', b'<STOP>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5Bgb43QMVpG"
      },
      "source": [
        "__Question 2__: what is the difference between __tag_dictionary__ created in the cell above, and __tag_dictionary__ created before that.<br>\n",
        "\n",
        "In the second tag_dictionary, we have only our three selected entities, namely the:\n",
        "- Skills\n",
        "- Companies\n",
        "- Degree\n",
        "\n",
        "but slight various of them with a starting character of either U, B, I, L which comes from our BILUO Schemes. This is what we rather want.\n",
        "\n",
        "<br>\n",
        "\n",
        "Next, we take the first sentence from the test data, and annotate this sentence using __to_tagged_string__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nie8-FX99JB7",
        "outputId": "ea95d6ff-e708-468e-ddec-3894445e6ffa"
      },
      "source": [
        "for i in range(1, 10):\n",
        "  print(corpus.test[-i].to_tagged_string())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ", <I-Skills> Catia <I-Skills> V6 <I-Skills> , <I-Skills> Ansys <L-Skills>\n",
            "Sri Venkateswara College Of Engineering - Chennai , Tamil Nadu 2012 to 2016 SKILLS ANSYS <B-Skills> ( <I-Skills> Less <I-Skills> than <I-Skills> 1 <I-Skills> year <I-Skills> ) <I-Skills> , <I-Skills> CATIA <I-Skills> ( <I-Skills> Less <I-Skills> than <I-Skills> 1 <I-Skills> year <I-Skills> ) <I-Skills> , <I-Skills> CREO <I-Skills> ( <I-Skills> Less <I-Skills> than <I-Skills> 1 <I-Skills> year <I-Skills> ) <I-Skills> , <I-Skills> PARAMETRIC <I-Skills> ( <I-Skills> Less <I-Skills> than <I-Skills> 1 <I-Skills> year <I-Skills> ) <I-Skills> , <I-Skills> PYTHON <I-Skills> ( <I-Skills> Less <I-Skills> than <I-Skills> 1 <I-Skills> year <I-Skills> ) <I-Skills> , <I-Skills> Selenium <I-Skills> , <I-Skills> Selenium <I-Skills> Webdriver <I-Skills> , <I-Skills> Testing <I-Skills> , <I-Skills> Functional <I-Skills> Testing <I-Skills> , <I-Skills> Automation <I-Skills> Testing <I-Skills> , <I-Skills> Regression <I-Skills> Testing <I-Skills> , <I-Skills> Quality <I-Skills> Assurance <L-Skills> ADDITIONAL INFORMATION TECHNICAL SKILLS • <B-Skills> Languages <I-Skills> - <I-Skills> Python <I-Skills> • <I-Skills> Software <I-Skills> / <I-Skills> Tools <I-Skills> - <I-Skills> Selenium <I-Skills> , <I-Skills> WAF <I-Skills> , <I-Skills> Sauce <I-Skills> Labs <I-Skills> , <I-Skills> Jenkins <I-Skills> , <I-Skills> Creo <I-Skills> parametric <I-Skills> 2.0 <I-Skills>\n",
            "PERFORMANCE ACHIEVEMENTS • Won The Rising Star Award for the year 2017 EDUCATION Bachelor <B-Degree> of <I-Degree> Engineering <I-Degree> in <I-Degree> Automobile <I-Degree> Engineering <L-Degree> https://www.indeed.com/r/Arun-Elumalai/26575d617d50ea04?isid=rex-download&ikw=download-top&co=IN\n",
            "Tested manual and auto enrollment of offers , cashback offers .\n",
            "5 .\n",
            "Tested various functionalities of credit card life cycle like account boarding , embossing , account / card transfer , replacement and reissue of cards .\n",
            "Performed UI Testing in First Apply and First Online 4 .\n",
            "3 .\n",
            "Automated manual scripts in Regression Testing and Executing the same using Selenium Web driver through Sauce Labs .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okNgtxrjMVpH"
      },
      "source": [
        "__Question 3__: Why is not every word annotated?<br>\n",
        "\n",
        "Because now we are predicting only for our three labels Degree, Companies and Skills. Since not every word is one of these labels, not every word gets annotated.\n",
        "\n",
        "<br>\n",
        "\n",
        "How do you explain the difference to the result from __to_tagged_string__ applied to one sentence from the wiki ner corpus?\n",
        "\n",
        "When we applied the to_tagged_string, the annotations we appended to the end of the string and then for every single word were the entity label was found, flair is showing which words for one entity have which BILUO letter, to see where the entity is starting and ending. In this way we can easily detect how many entities in a sentence were found. "
      ]
    }
  ]
}