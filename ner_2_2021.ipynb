{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ner_2_2021.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yjL03CKOoEk"
      },
      "source": [
        "Getting started with Spacy<br>\n",
        "Import data.<br>\n",
        "We repeat the preprocessing from the previous homework."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOxAAu3cOm9D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c49cc8c-a36d-47fe-9697-3e54147894aa"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOc5LrSCPSEC"
      },
      "source": [
        "import os"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jd_X8KyOPWlJ"
      },
      "source": [
        "os.chdir( \"/content/gdrive/MyDrive/flair\" ) "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBNxx2f8Pcmq"
      },
      "source": [
        "path_to_data = os.getcwd() + '/Entity Recognition in Resumes.json'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUKwMlfkaR2n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a436a651-efed-4913-9797-de7fc44bd184"
      },
      "source": [
        "myfile = open( path_to_data, \"r\", encoding = \"utf-8\" )\n",
        "\n",
        "imported_data = []\n",
        "\n",
        "for datum in myfile:\n",
        "    \n",
        "    # TODO process data\n",
        "    imported_data.append(datum)\n",
        "\n",
        "myfile.close()\n",
        "\n",
        "# TODO print first line\n",
        "print(imported_data[0])\n",
        "\n",
        "# TODO print how many resumees were read in\n",
        "print(len(imported_data))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"content\": \"Afreen Jamadar\\nActive member of IIIT Committee in Third year\\n\\nSangli, Maharashtra - Email me on Indeed: indeed.com/r/Afreen-Jamadar/8baf379b705e37c6\\n\\nI wish to use my knowledge, skills and conceptual understanding to create excellent team\\nenvironments and work consistently achieving organization objectives believes in taking initiative\\nand work to excellence in my work.\\n\\nWORK EXPERIENCE\\n\\nActive member of IIIT Committee in Third year\\n\\nCisco Networking -  Kanpur, Uttar Pradesh\\n\\norganized by Techkriti IIT Kanpur and Azure Skynet.\\nPERSONALLITY TRAITS:\\n• Quick learning ability\\n• hard working\\n\\nEDUCATION\\n\\nPG-DAC\\n\\nCDAC ACTS\\n\\n2017\\n\\nBachelor of Engg in Information Technology\\n\\nShivaji University Kolhapur -  Kolhapur, Maharashtra\\n\\n2016\\n\\nSKILLS\\n\\nDatabase (Less than 1 year), HTML (Less than 1 year), Linux. (Less than 1 year), MICROSOFT\\nACCESS (Less than 1 year), MICROSOFT WINDOWS (Less than 1 year)\\n\\nADDITIONAL INFORMATION\\n\\nTECHNICAL SKILLS:\\n\\n• Programming Languages: C, C++, Java, .net, php.\\n• Web Designing: HTML, XML\\n• Operating Systems: Windows […] Windows Server 2003, Linux.\\n• Database: MS Access, MS SQL Server 2008, Oracle 10g, MySql.\\n\\nhttps://www.indeed.com/r/Afreen-Jamadar/8baf379b705e37c6?isid=rex-download&ikw=download-top&co=IN\",\"annotation\":[{\"label\":[\"Email Address\"],\"points\":[{\"start\":1155,\"end\":1198,\"text\":\"indeed.com/r/Afreen-Jamadar/8baf379b705e37c6\"}]},{\"label\":[\"Links\"],\"points\":[{\"start\":1143,\"end\":1239,\"text\":\"https://www.indeed.com/r/Afreen-Jamadar/8baf379b705e37c6?isid=rex-download&ikw=download-top&co=IN\"}]},{\"label\":[\"Skills\"],\"points\":[{\"start\":743,\"end\":1140,\"text\":\"Database (Less than 1 year), HTML (Less than 1 year), Linux. (Less than 1 year), MICROSOFT\\nACCESS (Less than 1 year), MICROSOFT WINDOWS (Less than 1 year)\\n\\nADDITIONAL INFORMATION\\n\\nTECHNICAL SKILLS:\\n\\n• Programming Languages: C, C++, Java, .net, php.\\n• Web Designing: HTML, XML\\n• Operating Systems: Windows […] Windows Server 2003, Linux.\\n• Database: MS Access, MS SQL Server 2008, Oracle 10g, MySql.\"}]},{\"label\":[\"Graduation Year\"],\"points\":[{\"start\":729,\"end\":732,\"text\":\"2016\"}]},{\"label\":[\"College Name\"],\"points\":[{\"start\":675,\"end\":702,\"text\":\"Shivaji University Kolhapur \"}]},{\"label\":[\"Degree\"],\"points\":[{\"start\":631,\"end\":672,\"text\":\"Bachelor of Engg in Information Technology\"}]},{\"label\":[\"Graduation Year\"],\"points\":[{\"start\":625,\"end\":629,\"text\":\"2017\\n\"}]},{\"label\":[\"College Name\"],\"points\":[{\"start\":614,\"end\":622,\"text\":\"CDAC ACTS\"}]},{\"label\":[\"Degree\"],\"points\":[{\"start\":606,\"end\":611,\"text\":\"PG-DAC\"}]},{\"label\":[\"Companies worked at\"],\"points\":[{\"start\":438,\"end\":453,\"text\":\"Cisco Networking\"}]},{\"label\":[\"Email Address\"],\"points\":[{\"start\":104,\"end\":147,\"text\":\"indeed.com/r/Afreen-Jamadar/8baf379b705e37c6\"}]},{\"label\":[\"Location\"],\"points\":[{\"start\":62,\"end\":67,\"text\":\"Sangli\"}]},{\"label\":[\"Name\"],\"points\":[{\"start\":0,\"end\":13,\"text\":\"Afreen Jamadar\"}]}],\"extras\":null,\"metadata\":{\"first_done_at\":1527844872000,\"last_updated_at\":1537724086000,\"sec_taken\":0,\"last_updated_by\":\"BIQNZm4INNfvByMqkaVwVt6OZTv2\",\"status\":\"done\",\"evaluation\":\"CORRECT\"}}\n",
            "\n",
            "701\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxQojbZyPrkh"
      },
      "source": [
        "import json"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Axwa389baXDN"
      },
      "source": [
        "mapped_data = [ json.loads( datum ) for datum in imported_data  ]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2FaAX-xabje",
        "outputId": "155ab44b-c022-4e6f-cd78-b03c109e7099"
      },
      "source": [
        "## data conversion method\n",
        "def convert_data(data):\n",
        "    \"\"\"\n",
        "    Creates NER training data in Spacy format from JSON dataset\n",
        "    Outputs the Spacy training data which can be used for Spacy training.\n",
        "    \"\"\"\n",
        "    text = data['content']\n",
        "    entities = []\n",
        "    if data['annotation'] is not None:\n",
        "        for annotation in data['annotation']:\n",
        "            # only a single point in text annotation.\n",
        "            point = annotation['points'][0]\n",
        "            labels = annotation['label']\n",
        "            # handle both list of labels or a single label.\n",
        "            if not isinstance(labels, list):\n",
        "                labels = [labels]\n",
        "            for label in labels:\n",
        "                # dataturks indices are both inclusive [start, end] but spacy is not [start, end)\n",
        "                entities.append((point['start'], point['end'] + 1, label))\n",
        "    return (text, {\"entities\": entities})\n",
        "   \n",
        "## TODO using a loop or list comprehension, convert each resume in mapped_data using the convert function above, \n",
        "## storing the result\n",
        "converted_resumes = [convert_data(data) for data in mapped_data]\n",
        "## TODO print the number of resumes in converted resumes \n",
        "print(len(converted_resumes))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "701\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GapDFuttaqpk"
      },
      "source": [
        "# TODO filter out the resumees whose entities have no entries.\n",
        "converted_complete_resumees = [(text, ent) for text, ent in converted_resumes if len(ent[\"entities\"]) > 0]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-UWPUvfbo0l"
      },
      "source": [
        "Up until now, you could reuse the code from the previous notebook, now, something new comes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYhNshfBbl8d",
        "outputId": "a5959d6f-38a4-48c5-9054-f0244197c546"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "print(nlp)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<spacy.lang.en.English object at 0x7fdfe1219c10>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AqzhVdHb-xO"
      },
      "source": [
        "__nlp__ is Spacy's English language model. For this model, a pretrained NER-model exists."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDHFtX6Lb-CY",
        "outputId": "2dc754e5-b7ce-42ad-ffbb-ee823dfa37b2"
      },
      "source": [
        "ner = nlp.get_pipe('ner')\n",
        "labels = ner.labels\n",
        "print(labels)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBy9Uo20cZH2"
      },
      "source": [
        "__Question 1__: Explain the labels __GPE__, __FAC__, __NORP__.<br>\n",
        "Which of these labels from __ner__ do you think will Spacy recognize in the resumees?<br>\n",
        "\n",
        "Labels:\n",
        "  - GPE: Countries, Cities, States\n",
        "  - Buildings, Airports, Highways, Bridges, etc.\n",
        "  - Nationalities or religious or political Groups\n",
        "\n",
        "We will probably find GPE, especially cities or countries. Also ORG for the companies. Probably Time and Date for the period of company worked at. \n",
        "\n",
        "<br>\n",
        "__Task 1__: choose a resumee."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPNIMAW4249x",
        "outputId": "eb068954-41fa-4cfc-f0da-b2e6ea571af0"
      },
      "source": [
        "ner.pipe"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function EntityRecognizer.pipe>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yXayIFffE7F",
        "outputId": "f4b24e18-332a-4cd2-b1d6-85e8aa025e21"
      },
      "source": [
        "# TODO get a single resume text and print it out.\n",
        "restxt = converted_complete_resumees[27][0]\n",
        "## print it out, removing extraneous spaces\n",
        "print(\"\\n\".join(restxt.split('\\n\\n')))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Kasturika Borah\n",
            "Team Member - Cisco\n",
            "Bengaluru, Karnataka - Email me on Indeed: indeed.com/r/Kasturika-\n",
            "Borah/9e71468914b38ee8\n",
            "• Software Engineer with overall 3+ years of experience in Network Monitoring system tool (EM7,\n",
            "Quicksilver) Database tool (SQL, Maria DB) and reporting tool (Splunk) in all the releases.\n",
            "• Relevant experience as a Test engineer for the releases includes Functional testing as well as\n",
            "regression testing. Testing includes writing test cases, execute them and raise bugs.\n",
            "• Relevant 1+ years of experience in handling releases for EM7 with proper documentation, Power\n",
            "pack creation and Tar creation for Sprint releases.\n",
            "• Creating Splunk reports from last 6 months.\n",
            "• Competent technical person involved in requirement gathering, analysis, design and coding.\n",
            "• Experience in coding Python, SQL, and XML as per the requirement.\n",
            "• Have knowledge in Event generating using traps and Syslog's generator.\n",
            "• Exposure to Agile methodologies using Scrum Works framework, even handled scrum in the\n",
            "team\n",
            "• Strong problem-solver who can design solutions and assist developers with issues.\n",
            "• Excellent debugging and resolution skills.\n",
            "• Good communication and interpersonal skills.\n",
            "• Working as Software Engineer for Cisco System India Private Ltd under Capgemini India Pvt.\n",
            "Ltd.. From May 25th 2017 till nowl\n",
            "• Working as Software Engineer for Cisco System India Private Ltd under Randstad India Ltd.\n",
            "From Dec 15 2014 till 30th April.\n",
            "• Worked as Data Analyst for Fidelity India Financial Inc. from June 2013 till Oct 2014.\n",
            "• Worked as Billing Analyst for IBM Daksh from March 2013 to June 2013.\n",
            "Willing to relocate to: Bengaluru, Karnataka\n",
            "WORK EXPERIENCE\n",
            "Team Member\n",
            "Cisco -\n",
            "October 2017 to Present\n",
            "Environment: Splunk\n",
            "Technologies: SPL command\n",
            "Responsibilities\n",
            "• Involvement writing Splunk programming language and designing the report dashboard\n",
            "• Following Agile methodology\n",
            "• Develop the code on the design in splunk.\n",
            "• Unit Testing and code review\n",
            "Senior developer and tester\n",
            "https://www.indeed.com/r/Kasturika-Borah/9e71468914b38ee8?isid=rex-download&ikw=download-top&co=IN\n",
            "https://www.indeed.com/r/Kasturika-Borah/9e71468914b38ee8?isid=rex-download&ikw=download-top&co=IN\n",
            "\n",
            "Cisco -\n",
            "December 2014 to Present\n",
            "Environment: EM7 platform, Quicksilver, SQL, oracle Toad\n",
            "Technologies: Python coding, xml coding, SQL query writing\n",
            "Description\n",
            "Cisco Systems, Inc. (known as Cisco) is an American multinational technology conglomerate\n",
            "headquartered in San José, California, that develops, manufactures, and sells networking\n",
            "hardware, telecommunications equipment, and other high-technology services and products\n",
            "(www.cisco.com)\n",
            "Responsibilities\n",
            "1. Developer of individual task on each release by weekly\n",
            "• Need to do coding for new requirement.\n",
            "• Also need to do end to end testing of all the events including Traps and Syslogs.\n",
            "2. Database and Infrastructure Monitoring and Alerting related to device.\n",
            "3. Involvement in documentation of release notes and preparation of a Regression testing at the\n",
            "end of each release.\n",
            "Team Member\n",
            "Cisco -\n",
            "December 2014 to December 2017\n",
            "Environment: INFOVISTA (Vportal)\n",
            "Technologies: MS-Excel (sort, VLOOKUP), PPT\n",
            "Responsibilities\n",
            "• Involvement in generating performance reports for certain customers at the starting of every\n",
            "month\n",
            "• Gathering the data from the INFOVISTA portal and sort it out as per month in the excel and\n",
            "design the graphs for last consecutive\n",
            "• Responsible for each data uploaded to the excel sheet and reviewing it before delivering\n",
            "Fidelity national financial -\n",
            "June 2013 to October 2014\n",
            "Role: QA and Report handling for the team\n",
            "Technologies: MS-Excel (sort, VLOOKUP), PPT, MS-Outlook\n",
            "Responsibilities\n",
            "• Involvement in generating performance reports for the team at the end of each day and monthly\n",
            "based\n",
            "• Responsible for each data uploaded to the excel sheet and sending it to the team manager\n",
            "EDUCATION\n",
            "Compucom Insitute of Information Technology\n",
            "\n",
            "rajasthan University\n",
            "2012\n",
            "SKILLS\n",
            "Database (3 years), Python (3 years), Splunk (Less than 1 year), SQL (3 years), xml (3 years)\n",
            "ADDITIONAL INFORMATION\n",
            "TECHNICAL SKILLS\n",
            "• Programming Languages: Python, XML\n",
            "• Database: Maria-DB, sql\n",
            "• Cisco Monitoring Tools: EM7\n",
            "• Operating Systems: Windows/XP\n",
            "• Reporting Tools: Vportal, Splunk\n",
            "• Application & Web Servers: Sciencelogic (EM7), Syslog sender, Relay server.\n",
            "• Data Structure Knowledge: Intermediate\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ABsJBp0JtjN"
      },
      "source": [
        "Next, we let __nlp__ process that single resumee.<br>\n",
        "__Task 2__: print the results in __doc__. For each result, print the underlying text and the label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp961WAxfW8N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "795a2b92-f00d-44e6-de0a-79ab1388f5e4"
      },
      "source": [
        "doc = nlp(restxt)\n",
        "# TODO  Print the results in doc. For each result, print the text and the label.\n",
        "\n",
        "for token in doc:\n",
        "  if token.ent_type_:\n",
        "    print(f\"{token.text}: {token.ent_type_}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Kasturika: PERSON\n",
            "Borah: PERSON\n",
            "Bengaluru: GPE\n",
            "Karnataka: PERSON\n",
            "3: DATE\n",
            "+: DATE\n",
            "years: DATE\n",
            "Network: FAC\n",
            "Monitoring: FAC\n",
            "SQL: ORG\n",
            "Maria: PERSON\n",
            "DB: PERSON\n",
            "Sprint: ORG\n",
            "Creating: ORG\n",
            "Splunk: ORG\n",
            "last: DATE\n",
            "6: DATE\n",
            "months: DATE\n",
            "Python: GPE\n",
            "SQL: ORG\n",
            "XML: ORG\n",
            "Syslog: PERSON\n",
            "Scrum: PRODUCT\n",
            "Works: PRODUCT\n",
            "Cisco: ORG\n",
            "System: ORG\n",
            "India: ORG\n",
            "Private: ORG\n",
            "Ltd: ORG\n",
            "Capgemini: ORG\n",
            "India: ORG\n",
            "Pvt: ORG\n",
            ".: ORG\n",
            "\n",
            ": ORG\n",
            "May: DATE\n",
            "25th: DATE\n",
            "2017: DATE\n",
            "Cisco: ORG\n",
            "System: ORG\n",
            "India: ORG\n",
            "Private: ORG\n",
            "Ltd: ORG\n",
            "Randstad: ORG\n",
            "India: ORG\n",
            "Ltd.: ORG\n",
            "From: DATE\n",
            "Dec: DATE\n",
            "15: DATE\n",
            "2014: DATE\n",
            "Fidelity: ORG\n",
            "India: ORG\n",
            "Financial: ORG\n",
            "Inc.: ORG\n",
            "June: DATE\n",
            "2013: DATE\n",
            "2014: DATE\n",
            "Billing: ORG\n",
            "Analyst: ORG\n",
            "IBM: ORG\n",
            "Daksh: ORG\n",
            "March: DATE\n",
            "2013: DATE\n",
            "to: DATE\n",
            "June: DATE\n",
            "2013: DATE\n",
            "Bengaluru: GPE\n",
            "Karnataka: PERSON\n",
            "Team: ORG\n",
            "Member: ORG\n",
            "Cisco: GPE\n",
            "October: DATE\n",
            "2017: DATE\n",
            "SPL: ORG\n",
            "Following: ORG\n",
            "Agile: ORG\n",
            "Cisco: GPE\n",
            "December: DATE\n",
            "2014: DATE\n",
            "Quicksilver: GPE\n",
            "SQL: ORG\n",
            "Python: ORG\n",
            "xml: PERSON\n",
            "coding: PERSON\n",
            "SQL: ORG\n",
            "Cisco: ORG\n",
            "Systems: ORG\n",
            ",: ORG\n",
            "Inc.: ORG\n",
            "Cisco: GPE\n",
            "American: NORP\n",
            "San: GPE\n",
            "José: GPE\n",
            "California: GPE\n",
            "1: CARDINAL\n",
            "weekly: DATE\n",
            "Traps: WORK_OF_ART\n",
            "and: WORK_OF_ART\n",
            "Syslogs: WORK_OF_ART\n",
            "2: CARDINAL\n",
            "3: CARDINAL\n",
            "Cisco: GPE\n",
            "December: DATE\n",
            "2014: DATE\n",
            "to: DATE\n",
            "December: DATE\n",
            "2017: DATE\n",
            "every: DATE\n",
            "\n",
            ": DATE\n",
            "month: DATE\n",
            "INFOVISTA: ORG\n",
            "as: DATE\n",
            "per: DATE\n",
            "month: DATE\n",
            "graphs: NORP\n",
            "Fidelity: ORG\n",
            "June: DATE\n",
            "2013: DATE\n",
            "to: DATE\n",
            "October: DATE\n",
            "2014: DATE\n",
            "MS: ORG\n",
            "-: ORG\n",
            "Excel: ORG\n",
            "PPT: ORG\n",
            "MS: ORG\n",
            "-: ORG\n",
            "Outlook: ORG\n",
            "the: DATE\n",
            "end: DATE\n",
            "of: DATE\n",
            "each: DATE\n",
            "day: DATE\n",
            "monthly: DATE\n",
            "Compucom: ORG\n",
            "Insitute: ORG\n",
            "of: ORG\n",
            "Information: ORG\n",
            "SKILLS: ORG\n",
            "3: DATE\n",
            "years: DATE\n",
            "Python: GPE\n",
            "3: DATE\n",
            "years: DATE\n",
            "Less: DATE\n",
            "than: DATE\n",
            "1: DATE\n",
            "year: DATE\n",
            "SQL: ORG\n",
            "3: DATE\n",
            "years: DATE\n",
            "xml: PERSON\n",
            "3: DATE\n",
            "years: DATE\n",
            "TECHNICAL: ORG\n",
            "Programming: ORG\n",
            "Languages: ORG\n",
            ":: ORG\n",
            "Python: ORG\n",
            "XML: ORG\n",
            "Maria: PERSON\n",
            "-: PERSON\n",
            "DB: PERSON\n",
            "Operating: ORG\n",
            "Systems: ORG\n",
            "Syslog: PERSON\n",
            "Relay: PERSON\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xok9PDMplMNe"
      },
      "source": [
        "__Question 2__: How well did Spacy perform at recognizing the labels for this text?<br>\n",
        "When Spacy predicted the labels for this resumee, a pretrained model was used.\n",
        "\n",
        "Well spacy certainly found many good NER labels, but for example it doesn't understand that XML and SQL is not an ORG. Also PPT, MS and Excel was labeled as ORG. So Spacy has definetely problems with labeling the skills. We need to add a new label.\n",
        "\n",
        "<br>\n",
        "\n",
        "__Task 3__: print for this resumee the original labels and their corresponding text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8eIm9lXlznU",
        "outputId": "be968bbb-c999-4820-eb0d-6ff58be54d91"
      },
      "source": [
        "# TODO print for that resumee the original labels and their corresponding text.\n",
        "\n",
        "labeled_ents = converted_complete_resumees[27][1][\"entities\"]\n",
        "\n",
        "for ent in labeled_ents:\n",
        "  print(f\"{converted_complete_resumees[27][0][ent[0]:ent[1]]}: {ent[2]}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cisco: Companies worked at\n",
            "• Programming Languages: Python, XML\n",
            "• Database: Maria-DB, sql\n",
            "• Cisco Monitoring Tools: EM7\n",
            "• Operating Systems: Windows/XP\n",
            "• Reporting Tools: Vportal, Splunk\n",
            "• Application & Web Servers: Sciencelogic (EM7), Syslog sender, Relay server.\n",
            "• Data Structure Knowledge: Intermediate: Skills\n",
            "Database (3 years), Python (3 years), Splunk (Less than 1 year), SQL (3 years), xml (3 years)\n",
            ": Skills\n",
            "2012: Graduation Year\n",
            "rajasthan University\n",
            ": College Name\n",
            "Compucom Insitute of Information Technology\n",
            ": College Name\n",
            "Cisco: Companies worked at\n",
            "Team Member: Designation\n",
            "Cisco: Companies worked at\n",
            "Cisco: Companies worked at\n",
            "Cisco: Companies worked at\n",
            "Cisco: Companies worked at\n",
            "Team Member: Designation\n",
            "Bengaluru: Location\n",
            "Cisco: Companies worked at\n",
            "Cisco: Companies worked at\n",
            "Indeed: indeed.com/r/Kasturika-\n",
            "Borah/9e71468914b38ee8: Email Address\n",
            "Bengaluru: Location\n",
            "Cisco: Companies worked at\n",
            "Team Member: Designation\n",
            "Kasturika Borah: Name\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBHplvL2KHXO"
      },
      "source": [
        "__Question 3__: Compare the performance of the pretrained model __nlp__ and the true labels. Did Spacy perform well? If not, try to explain why.<br>\n",
        "\n",
        "It is not really compareable, since the labels of the both approaches are almost completely different. If we say that `companies worked at` = `ORG`, than we can say at least for this label spacy formed very well. Also for `PERSON`,  `DATE` and `TIME` spacy peforms well.\n",
        "\n",
        "<br>\n",
        "\n",
        "__Task 4__: Remember last homework? You chose three labels. Select all the resumees, in which all three labels appear."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wELEGAoEKDwf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f21cff0-f775-46cf-9291-04e0d785400f"
      },
      "source": [
        "# TODO fill in your chosen labels in chosen_entity_labels\n",
        "chosen_entity_labels = [ \"Degree\", \"Companies worked at\" , \"Skills\"]\n",
        "\n",
        "## this method gathers all resumes which have all of the chosen entites above.\n",
        "def gather_candidates(dataset,entity_labels):\n",
        "    candidates = list()\n",
        "    for resume in dataset:\n",
        "        res_ent_labels = list(zip(*resume[1][\"entities\"]))[2]\n",
        "        if set(entity_labels).issubset(res_ent_labels):\n",
        "            candidates.append(resume)\n",
        "    return candidates\n",
        "\n",
        "training_data = gather_candidates( converted_complete_resumees, chosen_entity_labels )\n",
        "print(\"Gathered {} training examples\".format(len(training_data)))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gathered 437 training examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSDI7dDDLBbW"
      },
      "source": [
        "__Task 5__: Next, we want to remove all other entities, since we only want to train NER for the three entities in __chosen_entity_labels__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DBr_VzxLNqe"
      },
      "source": [
        "## filter all annotation based on filter list\n",
        "def filter_ents(ents, filter):\n",
        "    filtered = [ent for ent in ents if ent[2] in filter]\n",
        "    return filtered\n",
        "\n",
        "## TODO use method above to remove all but relevant (chosen) entity annotations and store in X variable. X shall contain all\n",
        "## the resumees from training_data, but their entity annotations shall be filtered using the function from above.\n",
        "X = [(training_datum[0], {\"entities\": filter_ents(training_datum[1][\"entities\"], chosen_entity_labels)}) for training_datum in training_data]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZpyxtX6NteV"
      },
      "source": [
        "__Task 6__: Some resumees cause trouble. We filter these out with the following lines of code.<br>\n",
        "First, use __add_label__ to add your chosen labels to the __ner__ model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqwcgW_xX18e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f53416cf-3136-4c17-bb1a-c6da9a236fc8"
      },
      "source": [
        "from spacy.gold import GoldParse \n",
        "\n",
        "# TODO add your labels \n",
        "ner.add_label( chosen_entity_labels[0] )\n",
        "\n",
        "ner.add_label( chosen_entity_labels[1] )\n",
        "\n",
        "ner.add_label( chosen_entity_labels[2] )\n",
        "\n",
        "nlp.begin_training()\n",
        "\n",
        "good = []\n",
        "\n",
        "for item in X:\n",
        "  \n",
        "  text = nlp.make_doc( item[ 0 ] )\n",
        "\n",
        "  try:\n",
        "    \n",
        "    gold = GoldParse( text, entities = item[ 1 ][ \"entities\" ] )\n",
        "\n",
        "  except:\n",
        "\n",
        "    continue\n",
        "  \n",
        "  try:\n",
        "    \n",
        "    nlp.update( [ text ], [ gold ], drop = 0.3 )\n",
        "\n",
        "  except:\n",
        "\n",
        "    pass\n",
        "\n",
        "  else:\n",
        "\n",
        "    good.append( item )\n",
        "\n",
        "print( \"Number of good samples: \" + str( len( good ) ) )\n",
        "\n",
        "print( \"\" )\n",
        "\n",
        "print( \"\" )\n",
        "\n",
        "print( \"Number of bad sampples: \" + str( len( X ) - len( good ) ) )"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of good samples: 351\n",
            "\n",
            "\n",
            "Number of bad sampples: 86\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obZ93SLzyFIe"
      },
      "source": [
        "For a machine learning model, it is essential to be able to generalize. Only a model, that can generalize well is able to process new data in a meaningful way. Therefore, one usually separates the data set into two sets: the training set and the test set. The training set is used to train the model. The test set is used to evaluate the performance of the model on unseen data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0XVZPTyyh4T"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split( [ item[ 0 ] for item in good ], [ item[ 1 ] for item in good ], test_size = 0.3 )"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AcgIty-1otR"
      },
      "source": [
        "__Task 7__: Complete the following code. Shuffle __new_index__. Create the data sets __x_shuffled__ and __y_shuffled__. Use these to create minibatches, iterate over these minibatches, preprocess the data in a given minibatch using __nlp.make_doc__ and __GoldParse__. Employ __nlp.update__ to update the model using these preprocessed data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGLUALLr1rEp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da357ef5-f365-4abf-cad0-c8f6762960ee"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "nlp.begin_training()\n",
        "\n",
        "new_index = np.arange( len( x_train ) )\n",
        "\n",
        "x_data = np.array( x_train )\n",
        "\n",
        "y_data = np.array( y_train )\n",
        "\n",
        "for i in range( 20 ):\n",
        "  print(f\"Epoch: {i+1}\")\n",
        "  # TODO shuffle new_index\n",
        "  np.random.shuffle(new_index)\n",
        "\n",
        "  x_shuffled = x_data[new_index]\n",
        "\n",
        "  y_shuffled = y_data[new_index]\n",
        "\n",
        "  block_size = 35\n",
        "\n",
        "  for i in range(7):\n",
        "    \n",
        "    x_shuffled_batch = x_shuffled[block_size * i:block_size * (i + 1)]\n",
        "    y_shuffled_batch = y_shuffled[block_size * i:block_size * (i + 1)]\n",
        "   \n",
        "    texts = [nlp.make_doc(str(x)) for x in x_shuffled_batch]\n",
        "    golds = [GoldParse( text, entities = y_shuffled_batch[ \"entities\" ] ) for text, y_shuffled_batch in zip(texts, y_shuffled_batch)]\n",
        "    \n",
        "    nlp.update(texts, golds, drop = 0.3 )\n",
        "\n",
        "  # TODO\n",
        "  # divide the data in x_shuffled and y_shuffled into minibatches of identical size\n",
        "  # iterate over these minibatches\n",
        "  # preprocess the data in a minibatch using nlp.make_doc and GoldParse\n",
        "  # use these preprocessed data and nlp.update to train the model"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "Epoch: 2\n",
            "Epoch: 3\n",
            "Epoch: 4\n",
            "Epoch: 5\n",
            "Epoch: 6\n",
            "Epoch: 7\n",
            "Epoch: 8\n",
            "Epoch: 9\n",
            "Epoch: 10\n",
            "Epoch: 11\n",
            "Epoch: 12\n",
            "Epoch: 13\n",
            "Epoch: 14\n",
            "Epoch: 15\n",
            "Epoch: 16\n",
            "Epoch: 17\n",
            "Epoch: 18\n",
            "Epoch: 19\n",
            "Epoch: 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQSe2-BOYlWN"
      },
      "source": [
        "__Question 4__: Why did we shuffle the data?<br> \n",
        "Why did we employ mini batches?<br>\n",
        "Reasearch the term __epoch__ in machine learning. How many epochs of training do we employ?<br>\n",
        "\n",
        "Mini Batches: This is compareable to an SGD and a Mini-batch training. With single training example the gradient jumps alot around and with a minibatch we have a slightly more generalised gradient each training step and it is good practise to do mini batch training.\n",
        "\n",
        "For our training we employed 20 epochs. This is rather few, but can be sufficient enough. It could be that if we used more epochs, we overfit the model and if we use to few epochs we underfit the data. But this would need some further investigation.\n",
        "\n",
        "<br>\n",
        "\n",
        "__Task 8__: Next, we choose one resumee and print it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFGKoHVXY1pL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ff2d984-60e3-4457-b808-61e26f9c2788"
      },
      "source": [
        "n = 27\n",
        "resume = x_test[n]\n",
        "\n",
        "print( resume )"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sayed Shamim Azima\n",
            "NEOLEX CABLES as a JUNIOR QUALITY CONTROLLER & SALES MANAGER\n",
            "\n",
            "Mumbai, Maharashtra - Email me on Indeed: indeed.com/r/Sayed-Shamim-Azima/\n",
            "fd3544df79c46592\n",
            "\n",
            "Willing to relocate to: Mumbai, Maharashtra\n",
            "\n",
            "WORK EXPERIENCE\n",
            "\n",
            "JUNIOR QUALITY CONTROLLER & SALES MANAGER\n",
            "\n",
            "Neolex Cables -  India -\n",
            "\n",
            "October 2015 to September 2016\n",
            "\n",
            "Recruiting the companies for sales\n",
            "• Producing sales from them\n",
            "• Doing technical specification test for the copper, pvc, aluminium, wires etc.\n",
            "• Maintaining good relationship with the respected companies\n",
            "• Fulfill their needs & Generate Business from them.\n",
            "\n",
            "Sales Executive\n",
            "\n",
            "Supiro Services -  India -\n",
            "\n",
            "May 2015 to August 2015\n",
            "\n",
            "• To pitch policies to the provided customer \n",
            "• Selling of insurance to the clients\n",
            "• Generates leads from open market\n",
            "\n",
            "EDUCATION\n",
            "\n",
            "M.Sc. Physics\n",
            "\n",
            "Mumbai University\n",
            "\n",
            "SKILLS\n",
            "\n",
            "Quality Control, Quality Assurance\n",
            "\n",
            "https://www.indeed.com/r/Sayed-Shamim-Azima/fd3544df79c46592?isid=rex-download&ikw=download-top&co=IN\n",
            "https://www.indeed.com/r/Sayed-Shamim-Azima/fd3544df79c46592?isid=rex-download&ikw=download-top&co=IN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEY2UUvjJtjT"
      },
      "source": [
        "__Task 9__: we process this resumee using __nlp__. Print for all items in __doc.ents__ the predicted label and the corresponding text. Then print the correct labels and their corresponding text for that resumee with data from __y_test__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arAyrdyfbnQR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "815cf23b-7c0d-4648-d562-a67c02dee5d0"
      },
      "source": [
        "doc = nlp( resume )\n",
        "\n",
        "# TODO\n",
        "# print for all the items in doc.ents the predicted label and the corresponding text\n",
        "\n",
        "for token in doc:\n",
        "  if token.ent_type_:\n",
        "    print(f\"{token.text}: {token.ent_type_}\")\n",
        "# TODO\n",
        "# print the correct labels and their corresponding text for that resumee with data from y_test\n",
        "print(\"\\nCorrect Labels\\n\")\n",
        "for token in y_test[n][\"entities\"]:\n",
        "  print(f\"{token[2]}: {x_test[n][token[0]:token[1]]}\")\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Neolex: Companies worked at\n",
            "Cables: Companies worked at\n",
            "-: Companies worked at\n",
            " : Companies worked at\n",
            "India: Companies worked at\n",
            "-: Companies worked at\n",
            "\n",
            "\n",
            ": Companies worked at\n",
            "October: Companies worked at\n",
            "2015: Companies worked at\n",
            "Supiro: Companies worked at\n",
            "Services: Companies worked at\n",
            "-: Companies worked at\n",
            " : Companies worked at\n",
            "India: Companies worked at\n",
            "\n",
            "Correct Labels\n",
            "\n",
            "Skills: Quality Assurance\n",
            "Skills: Quality Control\n",
            "Degree: M.Sc. Physics\n",
            "Companies worked at: Supiro Services\n",
            "Companies worked at: Neolex Cables\n",
            "Companies worked at: NEOLEX CABLES\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTjZw4RVeB87"
      },
      "source": [
        "__Question 5__: What labels did the model predict correctly?<br> \n",
        "Where appeared problems?<br> \n",
        "How can you explain the problems?<br>\n",
        "\n",
        "The model performed very well on this task. It detected the skills, the degree and the companies worked for correctly. Really good. The problem is, that the companies worked for are found three times and especially with the `-` sign the model is performing very bad. Maybe we need to do more preprocessing before funneling the CV into the model training.\n",
        "\n",
        "<br>\n",
        "\n",
        "__Question 6__: We can evaluate the performance of the model using 4 metrics: the __Accuracy__, the __Precision__, the __Recall__ and __F1__.<br>\n",
        "Inform yourself on these metrics. How are they defined? Explain the concept of __True Positive__, __True Negative__, __False Positive__ and __False Negative__. Use these to define  the __Accuracy__, the __Precision__, the __Recall__ and __F1__, and also give the formula for each of these.<br>\n",
        "\n",
        "- Accuracy: (TP + TN) / (TP + TN + FP + FN)\n",
        "  - This measures how many of the labels we identified correctly as wrong and correctly as true\n",
        "- Precision: (TP) / (TP + FP)\n",
        "  - From all of the labels that were predicted positive, how many of them were actually correct? This therefore measures how precise our model predicts in regards of the being correct. \n",
        "\n",
        "- Recall (TP) / (TP + TN)\n",
        " - From all of the labels that are actually true, how many of the labels were predicted as false. When there is a high cost for False negatives, for example not detecting a cancer desease, we want to have a high as possible recall rate. \n",
        "\n",
        " - F1 2 * ( (Precision * Recall) / (Precision + Recall) )\n",
        "  - So if we were not bound by a specific requirement like at least 95% recall rate, then F1 Score is the balance for the optimal Recall and Precision value. This value gives the model with the best of both worlds precision and recall. It might also be better to use the F1 score for an imbalanced dataset, because then we are not so biased towards either precision or recall. \n",
        "\n",
        "<br>\n",
        "\n",
        "__Task 10__: Complete the following code. Call __make_bilou_df__ with a resume from the test set and store result in __bilou_df__ variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5M-ArsDtzFd",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "90e761d8-464f-4b83-ffd2-b7c00b70b14b"
      },
      "source": [
        "from spacy.gold import biluo_tags_from_offsets\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "## returns a pandas dataframe with tokens, prediction, and true (Gold Standard) annotations of tokens\n",
        "def make_bilou_df(nlp,resume):\n",
        "    \"\"\"\n",
        "    param nlp - a trained spacy model\n",
        "    param resume - a resume from our train or test set\n",
        "    \"\"\"\n",
        "    doc = nlp(resume[0])\n",
        "    bilou_ents_predicted = biluo_tags_from_offsets(doc, [(ent.start_char,ent.end_char,ent.label_)for ent in doc.ents])\n",
        "    bilou_ents_true = biluo_tags_from_offsets(doc, [(ent[0], ent[1], ent[2]) for ent in resume[1][\"entities\"]])\n",
        "\n",
        "    \n",
        "    doc_tokens = [tok.text for tok in doc]\n",
        "    bilou_df = pd.DataFrame()\n",
        "    bilou_df[\"Tokens\"] = doc_tokens\n",
        "    bilou_df[\"Tokens\"] = bilou_df[\"Tokens\"].str.replace(\"\\\\s+\",\"\") \n",
        "    bilou_df[\"Predicted\"] = bilou_ents_predicted\n",
        "    bilou_df[\"True\"] = bilou_ents_true\n",
        "    return bilou_df\n",
        "\n",
        "## TODO call method above with a resume from test set and store result in bilou_df variable.\n",
        "n = 9\n",
        "bilou_df = make_bilou_df( nlp, (x_test[n], y_test[n]) )\n",
        "display(bilou_df)  "
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tokens</th>\n",
              "      <th>Predicted</th>\n",
              "      <th>True</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sagar</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Kurada</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td></td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Manager</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1112</th>\n",
              "      <td>of</td>\n",
              "      <td>I-Skills</td>\n",
              "      <td>I-Skills</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1113</th>\n",
              "      <td>Quantitative</td>\n",
              "      <td>I-Skills</td>\n",
              "      <td>I-Skills</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1114</th>\n",
              "      <td>and</td>\n",
              "      <td>I-Skills</td>\n",
              "      <td>I-Skills</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1115</th>\n",
              "      <td>Qualitative</td>\n",
              "      <td>I-Skills</td>\n",
              "      <td>I-Skills</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1116</th>\n",
              "      <td>research</td>\n",
              "      <td>L-Skills</td>\n",
              "      <td>L-Skills</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1117 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            Tokens Predicted      True\n",
              "0            Sagar         O         O\n",
              "1           Kurada         O         O\n",
              "2                          O         O\n",
              "3          Manager         O         O\n",
              "4                -         O         O\n",
              "...            ...       ...       ...\n",
              "1112            of  I-Skills  I-Skills\n",
              "1113  Quantitative  I-Skills  I-Skills\n",
              "1114           and  I-Skills  I-Skills\n",
              "1115   Qualitative  I-Skills  I-Skills\n",
              "1116      research  L-Skills  L-Skills\n",
              "\n",
              "[1117 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04jh26HAJtjV"
      },
      "source": [
        "Inform yourself on the [BILUO](https://spacy.io/usage/linguistic-features#accessing-ner) scheme.<br>\n",
        "__Question 7__: Why do you think is it better to tag entities using this scheme (consider names of humans, descriptions of items in a shop)?<br>\n",
        "\n",
        "The BILUO scheme solves a problem that I previously mentioned. The companies worked for are marked as ORG for every single word. I can be regarded that each word of the companie name is an own ORG, but actually it is just one ORG. Also a human name consists of at least two words, therefore it is better to not tag it with two times PERSON.\n",
        "\n",
        "<br>\n",
        "\n",
        "__Task 11__: employ pandas dataframe api to get a subset where predicted and true labels are the same. Compute the accuracy using the formula you researched above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWRI3IfluPD7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d05ec7d2-d7d6-4534-c447-e7c166810b7a"
      },
      "source": [
        "## TODO bilou_df is a pandas dataframe. Use pandas dataframe api to get a subset where predicted and true are the same. \n",
        "same_df = bilou_df[bilou_df[\"Predicted\"] == bilou_df[\"True\"]]\n",
        "## TODO compute the accuracy \n",
        "accuracy = len(same_df) / len(bilou_df)\n",
        "\n",
        "print(\"Accuracy on one resume: \",accuracy)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on one resume:  0.8907788719785139\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1P4EgefioE-Q"
      },
      "source": [
        "The __accuracy__ is not 100%. Therefore, we want to have a look at those tokens, where the predicted and the true value differ.<br>\n",
        "__Task 12__: create a dataframe diff_df where the predicted values and the true values differ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdHFX1cMn-r6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "d4208bf1-fbaf-4912-d7bf-b31469fac855"
      },
      "source": [
        "# TODO create a dataframe diff_df where the predicted values and the true values differ\n",
        "diff_df = bilou_df[bilou_df[\"Predicted\"] != bilou_df[\"True\"]]\n",
        "display(diff_df)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tokens</th>\n",
              "      <th>Predicted</th>\n",
              "      <th>True</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>747</th>\n",
              "      <td>Gupta</td>\n",
              "      <td>O</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>748</th>\n",
              "      <td>power</td>\n",
              "      <td>O</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>749</th>\n",
              "      <td>infrastructure</td>\n",
              "      <td>O</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>750</th>\n",
              "      <td>limited</td>\n",
              "      <td>O</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>982</th>\n",
              "      <td>KIIT</td>\n",
              "      <td>B-Companies worked at</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1107</th>\n",
              "      <td>business</td>\n",
              "      <td>I-Skills</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1108</th>\n",
              "      <td>actionable</td>\n",
              "      <td>I-Skills</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1109</th>\n",
              "      <td></td>\n",
              "      <td>I-Skills</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1110</th>\n",
              "      <td>*</td>\n",
              "      <td>I-Skills</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1111</th>\n",
              "      <td>Understanding</td>\n",
              "      <td>I-Skills</td>\n",
              "      <td>B-Skills</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>122 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              Tokens              Predicted      True\n",
              "747            Gupta                      O         -\n",
              "748            power                      O         -\n",
              "749   infrastructure                      O         -\n",
              "750          limited                      O         -\n",
              "982             KIIT  B-Companies worked at         O\n",
              "...              ...                    ...       ...\n",
              "1107        business               I-Skills         O\n",
              "1108      actionable               I-Skills         O\n",
              "1109                               I-Skills         O\n",
              "1110               *               I-Skills         O\n",
              "1111   Understanding               I-Skills  B-Skills\n",
              "\n",
              "[122 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWvyiCokonUI"
      },
      "source": [
        "Since we only considered one resumee, we now make this comparison for the whole test set.<br>\n",
        "__Task 13__: Complete the following code for the computation of the overall accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "HpUixy6k0pS5",
        "outputId": "7df9b34c-ce3e-4ee0-efba-958b019d290b"
      },
      "source": [
        "x_test[0]"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Rahul Tayade\\nGlobal Production Support Lead, - Infosys Ltd (Technology Lead) - HSBC\\n\\nPune, Maharashtra - Email me on Indeed: indeed.com/r/Rahul-Tayade/ce40c3731cb69763\\n\\n• Total 12+ years of IT experience in the analysis, design, development, Testing support,\\nimplementation, CAT support and management of full life cycle applications, project coordination,\\nmanaging development and support projects.\\n• More than 7 years of experience on Project Leading support and maintenance project including\\nenhancements of the application.\\n• Over 4 years of Project Management experience on support/maintenance projects.\\n• Managed multiple applications with team more than 16 members.\\n• Involved in PM activities like dealing with customer, identifying new business, get new business,\\naccordingly raising quotes, and get PO approved from customer, track the work and bill customer\\nbased on Resource Utilization, Workforce management, manage project costing by providing\\ncost-effective solutions etc.\\n• Involve in SIX Sigma and Lean initiative to remove the unwanted NVA (non-value add) improve\\non costing and performance side\\n• Was deputed to client side UK-IPSWICH to deal with client and handle issues, achieve the client\\nconfidence by creating road map for improvement and improving performance and during that\\ntime customer satisfaction rating was increased to 4.9 from 4.4 out of 5.\\n• Handled responsibilities as the single point of contact for various projects, transitioning and\\noffshore coordinator.\\n• As a ASG Project lead involved in ASG activities like resource management, work allocation,\\nshift management, handling escalations, SLA performance and dashboard reporting, ITES Metric\\nreports, Highlight reports, coordinating and performing deployments, QMG audits, PMR activities\\netc.\\n• Practiced ITIL V3 processes during my tenure on application support projects which includes\\nService Transition, Service Operations (Incident management, Change Management, Problem\\nManagement) and Continual Service Improvements\\n• Handled effort estimation using Function Point (IFPUG Guidelines), cost estimation and planning\\nvarious projects.\\n• Prepared performance improvement plan on activities related to application performance.\\n• Managed and Delivered VDC Migrations, Database Migration projects.\\n• Capacity planning, work load and work force planning.\\n• Interact with the business analysts &amp; application leads to come up with technical designs\\nbased on the functional designs.\\n• Worked extensively on Oracle PL/SQL, UNIX, Scripting and have good interpersonal skills.\\n• Involved in Service transition and successfully completed all quality gates\\n• Documented and Managed DR activities successfully\\n• Involved in BCP planning and execution.\\n• Team mentoring and help team when needed\\n• Involved in escalations and resolve the issue smoothly by involving business stakeholders and\\nteam effectively.\\n\\nWORK EXPERIENCE\\n\\nGlobal Production Support Lead, - Infosys Ltd (Technology Lead)\\n\\nhttps://www.indeed.com/r/Rahul-Tayade/ce40c3731cb69763?isid=rex-download&ikw=download-top&co=IN\\n\\n\\nHSBC -  Pune, Maharashtra -\\n\\nJune 2016 to Present\\n\\nCurrently working in Global Standards IT under the CDD (Customer Due Diligence) program which\\nis responsible for tackling financial crime by implementing new tools to better understand their\\ncustomers.\\n\\nI lead a global 24*7 production support team for a number of Tier 1 KYC (Know Your Client)\\nFCR (Financial Crime Risk) applications and provide hands on application support to a user base\\nacross 35 countries in 42 markets, for HSBC and its subsidiaries First Direct and Marks &amp;\\nSpencer Bank.\\n\\nTasks:\\n• Mentoring team on Technical as well as Process front\\n• Implementing new processes\\n• Implementing a global 24*7 Support Model\\n• Supporting the rollout of new customer onboarding tool across 35 countries\\n• Supporting weekly release cycles\\n\\nResponsibilities:\\n• Offshore Leadership/People Management\\n• Service Delivery Management\\n• Stakeholder Management\\n• Relationship Management\\n• Service Recovery Management\\n• Production Support\\n• Incident Management\\n• Problem Management\\n• Change Management\\n• Release Management &amp; Support during releases\\n• Involve in Service Improvement Process for Faster and Smooth delivery\\n• Resolving technical issues relating to application from offshore\\n• Training &amp; Mentoring new resources for the Projects\\n• Documentation and manage Knowledge bank for team reference\\n• Documentation and execution of DR activities\\n\\nTechnology Lead\\n\\nHSBC -  Pune, Maharashtra -\\n\\nOctober 2015 to July 2016\\n\\nInvolved in Transition of application\\n• Preparation of in Knowledge Transfer plan from HSBC to Infosys,\\n• Managing KT schedules and made sure that it working as per plan\\n• Managing variance and made sure that portion left should get discussed as per updated plans.\\n• Mentoring new team on technical and process front\\n• Helping team where laps in KT plan or miscommunications\\n• Preparation and scheduling of Reverse KT plans and smooth execution of it\\n\\n\\n\\n• Managing stake holders and business and give them confidence for moving support to new\\nteam\\n• Made sure that this plan passed from all quality gates in order to start work from new team\\n• Documentation of all these Knowledge and Process\\n\\nProject Lead\\n\\nBritish Telecom UK -  Pune, Maharashtra -\\n\\nAugust 2010 to July 2015\\n\\nFPQ system is used for entry and reporting of quality data. It is national database for all quality\\nchecks. It is used to record field performance quality scores of resources and based on the entry\\nof it evaluate the quality scores. It is also used for score sheet and contractor management. Base\\non the score sheets auditors performs the transaction audits. FPQ also provides the transactional\\nand statistical reports.\\nV21 is crucial interface between the OSS and the network. It is based on Metasolv's component\\nOMS.\\nOMS: A Centralized Order Management system that allows changing processes and adding\\ninterfaces automatically\\n\\n• Off Shore Project Manager for various integration projects for the client.\\n• Requirement Understanding for future development and enhancement by interacting with\\nClient, E2E Solution Designers and other stake holders.\\n• Prepared application performance improvement plan such CSIP (Continuous Service\\nImprovement Plan), Get-well Plan.\\n• Providing estimation of effort and timescale for all the project deliveries\\n• Managing components deliveries impacted by various Releases\\n• Perform project planning, scheduling, monitoring, and reporting activities.\\n• Interface with the client team to update them on the issues, risks and status of the offshore\\ndelivery.\\n• Part of Application/Detail Design team to design high level and low level of the integration work.\\n• Ensure system is delivered within planned cost, timescale and resource budgets.\\n• Perform Release Management\\n• Effort Estimation, Cost estimation, Allocate work to the team, track and raise the queries and\\nresolve issues related to deliveries.\\n• Design, Development, Testing for various projects.\\n• Change Requests/Maintenance Release\\n• Resolving technical issues relating to application from offshore\\n• Training &amp; Mentoring new resources for the Projects.\\n• Oracle Database Migration, Application Migration to VDC environment.\\n• Completed VDC Migration (DaaS, CaaS, MaaS) for various application\\n\\nAlong-side the responsibilities mentioned above I was involved in various initiatives by client for\\nimprovement in team efficiency like Six Sigma and Lean management that saved lots of efforts\\nand in turns monetary benefits to customer.\\n\\nWe also developed solutions to various issues which reduced incidents count that also reduce\\nturnaround time for resolution of the issue that engineer faced. For which we have received Blue\\nRibbon Award.\\n\\n\\n\\nTeam Lead\\n\\nBritish Telecomm UK -  UK -\\n\\nJune 2006 to July 2010\\n\\nEWMP-Tacticals contains Robotic systems formerly with the FastTrack solutions Team within BT.\\nRobotic systems were developed with the purpose of reducing the manual/ repetitive work done\\nby the Field engineers. This work is automated by Robots and User interface interacting with\\nvarious systems like CSS, Work Manager and other components. It is bunch of 16 applications\\n\\n• Off Shore Project Manager for various integration projects for the client.\\n• Requirement Understanding for future development and enhancement by interacting with\\nClient, E2E Solution Designers and other stake holders.\\n• Prepared application performance improvement plan such CSIP (Continuous Service\\nImprovement Plan), GetWell Plan.\\n• Providing estimation of effort and timescale for all the project deliveries\\n• Managing components deliveries impacted by various Releases\\n• Perform project planning, scheduling, monitoring, and reporting activities.\\n• Interface with the client team to update them on the issues, risks and status of the offshore\\ndelivery.\\n• Part of Application/Detail Design team to design high level and low level of the integration work.\\n• Ensure system is delivered within planned cost, timescale and resource budgets.\\n• Perform Release Management\\n• Effort Estimation, Cost estimation, Allocate work to the team, track and raise the queries and\\nresolve issues related to deliveries.\\n• Design, Development, Testing for various projects.\\n• Change Requests/Maintenance Release\\n• Resolving technical issues relating to application from offshore\\n• Training &amp; Mentoring new resources for the Projects.\\n• Oracle Database Migration, Application Migration to VDC environment.\\n• Completed VDC Migration (DaaS, CaaS, MaaS) for various application before EOSL (End of life\\ncycle of the application)\\n\\nFor one of the legacy application (FastQ) , we have provided L3 support, application basically\\ndeveloped Oracle HTTPS based and Tomcat as Service to handle it.\\nDuring change in business level we have done development and changes for this application for\\nwhich Customer gives me Start Team player award as this was legacy application and before this\\nchange there was no change done for more than 6 years.\\n\\nSystem Analyst\\n\\nGE Countrywide, Lending and Repay Management -\\n\\nMay 2005 to April 2006\\n\\nLeasing functionality as per the standard defined by financial institution. It includes all the\\nfunctionality from Creation of Group, Company, and Creation of Trenches, Disbursing Loan\\nAgreement No. Capitalizing LAN.\\nIn Repayment Management System will take care of Installment receipt, Write-off cases, Charges\\nfor delinquent cases, Foreclosure of LAN etc\\n\\n\\n\\nThis application was developed from Scratch so involved in every part of application life cycle\\nfrom requirement gathering till UAT support before go-live\\n\\n• Database design, GUI design.\\n• Development of database objects like procedure packages, and development of GUI.\\n• Design and developed customized report as per client requirement.\\n• Component testing.\\n• End to end test support.\\n• UAT support and post deployment support.\\n\\nEDUCATION\\n\\nBachelor of Engineering in ELECTRONICS AND TELECOMMUNICATION\\n\\nAmravati University -  Amravati, Maharashtra\\n\\n1997 to 1999\\n\\nDiploma in INDUSTRIAL ELECTRONICS\\n\\nTechnical Board of Education Bombay -  Mumbai, Maharashtra\\n\\n1993 to 1996\\n\\nSKILLS\\n\\nMENTORING (10+ years), SCHEDULING (9 years), ORACLE (9 years), SOLUTIONS (9 years),\\nBENEFITS (4 years)\\n\\nADDITIONAL INFORMATION\\n\\nI.T KNOWLEDGE &amp; SKILLS\\nProgramming Languages: VB script, C, C++, Visual Basic, COM/DCOM\\nDatabases: Oracle, SQL,\\nDevelopment Tools: Toad, PL/SQL Developer, Crystal Report, Putty,Clarify, GSD, RTC, JIRA,\\nConfluence, VSS, PVCS,\\nWeb Servers: IIS, Oracle HTTP Server on Windows, Weblogic 10.3, WebSphere, Apache Tomcat,\\nMTS\\nBatch Scheduling: Control-M, Cron0Jobs, Windows Schedulers\\nOperating Systems: Windows […] Pro &amp; Server/XP Home &amp; Professional/2003 Server/\\nVista/7 Pro &amp; Enterprise/8/10, MS-DOS, UNIX, Linux\\nWindows Packages: Microsoft Office: (Word, Excel, Access, Outlook and PowerPoint), Oracle\\nDBMS, Internet Explorer, Netscape, Lotus Notes, Adobe Flash, Photoshop, and CITRIX Metaframe\\n1.8/XP\\nTransferable Skills: Excellent business skills, project management, presentation, interpersonal,\\ncommunication and report writing skills, Team Mentoring\\n\\nOTHER SKILLS\\n• Sound customer-facing skills: drive demos, status calls, issues &amp; escalation handling and\\nprovide solutions which benefits the business and customer\\n\\n\\n\\n• Dynamic Team leader, strong resource management, team building skills and conflict\\nmanagement. Strong in result oriented service delivery to the customer\\n• Excellent cross-vendor communication skills\\n• Strong Analytical &amp; problem solving ability and proactively drive opportunities to resolution\\nwithout supervision\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bidqT9GjovAg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "843118af-f607-4401-e55c-abf5d1e9ab13"
      },
      "source": [
        "doc_accuracy = []\n",
        "\n",
        "for i in range( len( x_test ) ):\n",
        "\n",
        "  resume = (x_test[i], y_test[i])\n",
        "\n",
        "  bilou_df = make_bilou_df(nlp,resume)\n",
        "\n",
        "  same_df = bilou_df[bilou_df[\"Predicted\"] == bilou_df[\"True\"]]\n",
        "\n",
        "  doc_accuracy.append( len(same_df) / len(bilou_df) )\n",
        "\n",
        "total_acc = np.mean( doc_accuracy )\n",
        "print(\"Accuracy: \",total_acc)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.8515458526212976\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cL7GE3RQpzbg"
      },
      "source": [
        "So we got an __accuracy__ of about 90% on average. This is quite good considering, that we only considered about 300 cases for training.<br>\n",
        "__Task 14__: Next, we want to find out, what the model did, when it went wrong. We only consider 5 resumees.<br>\n",
        "Complete the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y02Pkxriq8GM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8ecc118d-2d5c-4373-a261-8c60c3fb51d0"
      },
      "source": [
        "for i in range( 5 ):\n",
        "\n",
        "  resume = (x_test[i+3], y_test[i+3])\n",
        "\n",
        "  bilou_df = make_bilou_df(nlp,resume)\n",
        "\n",
        "  difference_df = bilou_df[bilou_df[\"Predicted\"] != bilou_df[\"True\"]]\n",
        "\n",
        "  # TODO: print, where the labels from Spacy and the annotation differ. Print the text, the predicted and the true labels.\n",
        "  display(difference_df)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tokens</th>\n",
              "      <th>Predicted</th>\n",
              "      <th>True</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Portfolio</td>\n",
              "      <td>O</td>\n",
              "      <td>B-Skills</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Management</td>\n",
              "      <td>O</td>\n",
              "      <td>L-Skills</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Axis</td>\n",
              "      <td>O</td>\n",
              "      <td>B-Companies worked at</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Bank</td>\n",
              "      <td>O</td>\n",
              "      <td>L-Companies worked at</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>Axis</td>\n",
              "      <td>O</td>\n",
              "      <td>B-Companies worked at</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>885</th>\n",
              "      <td>Portfolio</td>\n",
              "      <td>O</td>\n",
              "      <td>B-Skills</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>886</th>\n",
              "      <td>Management</td>\n",
              "      <td>O</td>\n",
              "      <td>L-Skills</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>888</th>\n",
              "      <td>Analytics</td>\n",
              "      <td>O</td>\n",
              "      <td>U-Skills</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>921</th>\n",
              "      <td>Axis</td>\n",
              "      <td>O</td>\n",
              "      <td>B-Companies worked at</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>922</th>\n",
              "      <td>Bank</td>\n",
              "      <td>O</td>\n",
              "      <td>L-Companies worked at</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>64 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         Tokens Predicted                   True\n",
              "6     Portfolio         O               B-Skills\n",
              "7    Management         O               L-Skills\n",
              "10         Axis         O  B-Companies worked at\n",
              "11         Bank         O  L-Companies worked at\n",
              "53         Axis         O  B-Companies worked at\n",
              "..          ...       ...                    ...\n",
              "885   Portfolio         O               B-Skills\n",
              "886  Management         O               L-Skills\n",
              "888   Analytics         O               U-Skills\n",
              "921        Axis         O  B-Companies worked at\n",
              "922        Bank         O  L-Companies worked at\n",
              "\n",
              "[64 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tokens</th>\n",
              "      <th>Predicted</th>\n",
              "      <th>True</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>SAP</td>\n",
              "      <td>B-Companies worked at</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>-</td>\n",
              "      <td>I-Companies worked at</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td></td>\n",
              "      <td>I-Companies worked at</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>Bengaluru</td>\n",
              "      <td>L-Companies worked at</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>Electrical</td>\n",
              "      <td>I-Degree</td>\n",
              "      <td>L-Degree</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>409</th>\n",
              "      <td>Interfaced</td>\n",
              "      <td>I-Skills</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>410</th>\n",
              "      <td>With</td>\n",
              "      <td>I-Skills</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>411</th>\n",
              "      <td>In</td>\n",
              "      <td>I-Skills</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>412</th>\n",
              "      <td>touch</td>\n",
              "      <td>I-Skills</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>413</th>\n",
              "      <td>SCADA</td>\n",
              "      <td>L-Skills</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>267 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         Tokens              Predicted      True\n",
              "62          SAP  B-Companies worked at         -\n",
              "63            -  I-Companies worked at         O\n",
              "64               I-Companies worked at         O\n",
              "65    Bengaluru  L-Companies worked at         O\n",
              "100  Electrical               I-Degree  L-Degree\n",
              "..          ...                    ...       ...\n",
              "409  Interfaced               I-Skills         O\n",
              "410        With               I-Skills         O\n",
              "411          In               I-Skills         O\n",
              "412       touch               I-Skills         O\n",
              "413       SCADA               L-Skills         O\n",
              "\n",
              "[267 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tokens</th>\n",
              "      <th>Predicted</th>\n",
              "      <th>True</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>Ingeral</td>\n",
              "      <td>O</td>\n",
              "      <td>B-Companies worked at</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Sales</td>\n",
              "      <td>O</td>\n",
              "      <td>I-Companies worked at</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>Corporation</td>\n",
              "      <td>O</td>\n",
              "      <td>L-Companies worked at</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>ltd</td>\n",
              "      <td>I-Companies worked at</td>\n",
              "      <td>L-Companies worked at</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>-</td>\n",
              "      <td>I-Companies worked at</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td></td>\n",
              "      <td>I-Companies worked at</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>Mumbai</td>\n",
              "      <td>L-Companies worked at</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>Management</td>\n",
              "      <td>I-Degree</td>\n",
              "      <td>L-Degree</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td></td>\n",
              "      <td>L-Degree</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>Fluent</td>\n",
              "      <td>O</td>\n",
              "      <td>B-Skills</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>English</td>\n",
              "      <td>O</td>\n",
              "      <td>L-Skills</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Tokens              Predicted                   True\n",
              "28      Ingeral                      O  B-Companies worked at\n",
              "29        Sales                      O  I-Companies worked at\n",
              "30  Corporation                      O  L-Companies worked at\n",
              "65          ltd  I-Companies worked at  L-Companies worked at\n",
              "66            -  I-Companies worked at                      O\n",
              "67               I-Companies worked at                      O\n",
              "68       Mumbai  L-Companies worked at                      O\n",
              "83   Management               I-Degree               L-Degree\n",
              "84                            L-Degree                      O\n",
              "95       Fluent                      O               B-Skills\n",
              "96      English                      O               L-Skills"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tokens</th>\n",
              "      <th>Predicted</th>\n",
              "      <th>True</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Ltd</td>\n",
              "      <td>O</td>\n",
              "      <td>B-Companies worked at</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Mumbai</td>\n",
              "      <td>O</td>\n",
              "      <td>L-Companies worked at</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Pepperl</td>\n",
              "      <td>O</td>\n",
              "      <td>B-Companies worked at</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>&amp;</td>\n",
              "      <td>O</td>\n",
              "      <td>I-Companies worked at</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Fuchs</td>\n",
              "      <td>O</td>\n",
              "      <td>I-Companies worked at</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1083</th>\n",
              "      <td>Management</td>\n",
              "      <td>I-Skills</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1084</th>\n",
              "      <td>Technical</td>\n",
              "      <td>I-Skills</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1085</th>\n",
              "      <td>Solution</td>\n",
              "      <td>I-Skills</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1086</th>\n",
              "      <td>Service</td>\n",
              "      <td>I-Skills</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1087</th>\n",
              "      <td>Delivery</td>\n",
              "      <td>L-Skills</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>106 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          Tokens Predicted                   True\n",
              "3            Ltd         O  B-Companies worked at\n",
              "4         Mumbai         O  L-Companies worked at\n",
              "9        Pepperl         O  B-Companies worked at\n",
              "10             &         O  I-Companies worked at\n",
              "11         Fuchs         O  I-Companies worked at\n",
              "...          ...       ...                    ...\n",
              "1083  Management  I-Skills                      O\n",
              "1084   Technical  I-Skills                      O\n",
              "1085    Solution  I-Skills                      O\n",
              "1086     Service  I-Skills                      O\n",
              "1087    Delivery  L-Skills                      O\n",
              "\n",
              "[106 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tokens</th>\n",
              "      <th>Predicted</th>\n",
              "      <th>True</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>Microsoft</td>\n",
              "      <td>B-Companies worked at</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>Student</td>\n",
              "      <td>I-Companies worked at</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>Partner</td>\n",
              "      <td>I-Companies worked at</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td></td>\n",
              "      <td>I-Companies worked at</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>MICROSOFT</td>\n",
              "      <td>L-Companies worked at</td>\n",
              "      <td>U-Companies worked at</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>567</th>\n",
              "      <td>10</td>\n",
              "      <td>O</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>/</td>\n",
              "      <td>O</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>569</th>\n",
              "      <td>8.1/</td>\n",
              "      <td>O</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>570</th>\n",
              "      <td>7</td>\n",
              "      <td>O</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>571</th>\n",
              "      <td></td>\n",
              "      <td>O</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>112 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Tokens              Predicted                   True\n",
              "70   Microsoft  B-Companies worked at                      O\n",
              "71     Student  I-Companies worked at                      O\n",
              "72     Partner  I-Companies worked at                      O\n",
              "73              I-Companies worked at                      O\n",
              "74   MICROSOFT  L-Companies worked at  U-Companies worked at\n",
              "..         ...                    ...                    ...\n",
              "567         10                      O                      -\n",
              "568          /                      O                      -\n",
              "569       8.1/                      O                      -\n",
              "570          7                      O                      -\n",
              "571                                 O                      -\n",
              "\n",
              "[112 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0r21TdjPrt7P"
      },
      "source": [
        "__Question 8__: What was predicted, when the prediction differed from the true label?<br>\n",
        "What do you think is necessary for computing the accuracy on token level?<br> \n",
        "What is the advantage of computing the accuracy on token level?<br>\n",
        "\n",
        "Most of the time when the prediction is wrong, the model predicted an `O`，which stands for `Token is outside an entity`, which means that no entity could have been identified. Often is also the true label an `O` and the model predicted as already said for one entity for every single word this entity and therefore the prediction is wrong. When we predict on token level we need to consider that one token can have multiple words, but it's better and more precise, because it reflects the real world more. \n",
        "\n",
        "<br>\n",
        "\n",
        "__Task 15__: Complete the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZGOVg2U20V1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "f3169156-3341-41af-e4ea-6de2205cb4ac"
      },
      "source": [
        "## TODO cycle through chosen_entity_labels and calculate metrics for each entity using test data\n",
        "data = []\n",
        "for label in chosen_entity_labels:\n",
        "    ## variables to store results for all resumes for one entity type\n",
        "    true_positives = 0\n",
        "    false_positives = 0\n",
        "    false_negatives = 0\n",
        "    for i in range( len( x_test ) ):\n",
        "        ## use make_bilou_df on each resume in our test set, and calculate for each entity true and false positives,\n",
        "        ## and false negatives. \n",
        "\n",
        "        resume = (x_test[i], y_test[i])\n",
        "        \n",
        "        tres_df = make_bilou_df(nlp,resume)\n",
        "\n",
        "        ## calculate true false positives and false negatives for each resume\n",
        "        \n",
        "        tp = tres_df[(tres_df[\"Predicted\"] == tres_df[\"True\"]) & (tres_df[\"Predicted\"].str.contains(label))] \n",
        "        \n",
        "        fp = tres_df[(tres_df[\"Predicted\"] != tres_df[\"True\"]) & (tres_df[\"Predicted\"].str.contains(label))] \n",
        "        \n",
        "        fn = tres_df[(tres_df[\"Predicted\"] != tres_df[\"True\"]) & ~(tres_df[\"Predicted\"].str.contains(label))] \n",
        "\n",
        "        ## aggregate result for each resume to totals\n",
        "        true_positives = true_positives + len(tp)\n",
        "        false_positives = false_positives + len(fp)\n",
        "        false_negatives = false_negatives + len(fn)\n",
        "\n",
        "    print(\"For label '{}' tp: {} fp: {} fn: {}\".format(label,true_positives,false_positives,false_negatives))\n",
        "    \n",
        "    ## TODO Use the formulas you learned to calculate metrics and print them out\n",
        "    ## also: prevent division by zero without raising errors. Explain your choice\n",
        "    e = np.finfo(float).eps\n",
        "    \n",
        "    precision = (true_positives + e) / (true_positives + false_positives + e)\n",
        "    recall = (true_positives + e) / (true_positives + false_negatives + e)\n",
        "    f1 = 2 * ((precision * recall + e) / (precision + recall + e))\n",
        "    \n",
        "    row = [label,precision,recall,f1]\n",
        "    data.append(row)\n",
        "\n",
        "## make pandas dataframe with metrics data. Use the chosen entity labels as an index, and the metric names as columns. \n",
        "metric_df = pd.DataFrame( data, columns = [ \"Label\", \"Precision\", \"Recall\", \"F1\" ] )\n",
        "display(metric_df)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For label 'Degree' tp: 186 fp: 853 fn: 6925\n",
            "For label 'Companies worked at' tp: 536 fp: 912 fn: 6866\n",
            "For label 'Skills' tp: 864 fp: 2770 fn: 5008\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Degree</td>\n",
              "      <td>0.179018</td>\n",
              "      <td>0.026157</td>\n",
              "      <td>0.045644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Companies worked at</td>\n",
              "      <td>0.370166</td>\n",
              "      <td>0.072413</td>\n",
              "      <td>0.121130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Skills</td>\n",
              "      <td>0.237755</td>\n",
              "      <td>0.147139</td>\n",
              "      <td>0.181780</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Label  Precision    Recall        F1\n",
              "0               Degree   0.179018  0.026157  0.045644\n",
              "1  Companies worked at   0.370166  0.072413  0.121130\n",
              "2               Skills   0.237755  0.147139  0.181780"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMKMlJswAqUZ"
      },
      "source": [
        "__Question 9__: Explain from these statistics how well __nlp__ performs.<br>\n",
        "\n",
        "Based on these statistics, nlp performs really bad. \n",
        "\n",
        "For example Degree:\n",
        "\n",
        "- Precision:\n",
        "From all of the labels that were predicted as Degree were only 18% actually a Degree. This of course depends on that there is only 1 or 2 degrees, but the prediction predicts Degree on every single word for the degree institution. \n",
        "\n",
        "- Recall:\n",
        "From our actual positive degrees, nlp is only capable to find 2,6% of these labels as correct. This is of course very bad and we cannot trust this model at all. We need to find a better way than labeling every single word with the same Token. \n",
        "\n",
        "- F1:\n",
        "Due to Precision and Recall being very bad, especially Recall, F1 can only be bad too, because it's like the balanace of these two. Therefore we have a score somewhat in the middle of these two.\n",
        "\n",
        "For Companis worked at and Skills applies almost the same logic, expect that they performed slightly better for the Precision, this could be due to the words of Companies and Skills are longer, but the False Positives are lower, that's why the Precision is higher, even though it does not work as well.\n",
        "\n",
        "<br>\n",
        "\n",
        "__Task 16__: Compute for each metric (Precision, Recall, F1) the mean values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_gEZTQy5KTr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a28a2910-c51b-4ec1-8f62-bf8db7bc1e94"
      },
      "source": [
        "for label in [ \"Precision\", \"Recall\", \"F1\" ]:\n",
        "    \n",
        "    # Compute mean and print\n",
        "    print(f\"Mean {label}: {metric_df[label].mean()}\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Precision: 0.2623128577072971\n",
            "Mean Recall: 0.08190282822142089\n",
            "Mean F1: 0.11618468124940066\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q1rvvhfBoN9"
      },
      "source": [
        "__Question 10__: What do you learn, when you compare the performance of the model on the token level with the performance of the model on the global level from above?<br>\n",
        "\n",
        "We learn that the global metrics for this kind of task can not really be trusted, because we need to elaborate on all of the different entities for themselves. A few high scoring entities can enhance the global score, even though some of the labels are really bad performing. We learn that we need to make a good evaluation on all of our wanted entities.\n",
        "\n",
        "<br>\n",
        "\n",
        "Next, we prepare data for flair."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vkzBYIlDbLe"
      },
      "source": [
        "train = [ [ x_train[ i ], y_train[ i ] ] for i in range( len( x_train ) ) ]\n",
        "\n",
        "test = [ [ x_test[ i ], y_test[ i ] ] for i in range( len( x_test ) ) ]"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgSt_twJJtjZ"
      },
      "source": [
        "__Task 17__: Complete the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBfdFr1qNKBv"
      },
      "source": [
        "# prepare data\n",
        "training_data_as_bilou = [make_bilou_df(nlp,res) for res in train]\n",
        "\n",
        "test_data_as_bilou = [make_bilou_df(nlp,res) for res in test]\n",
        "\n",
        "\n",
        "# set up paths\n",
        "path_to_training_file = os.getcwd() + \"/training_data.csv\"\n",
        "\n",
        "path_to_test_file = os.getcwd() + \"/test_data.csv\"\n",
        "\n",
        "\n",
        "\n",
        "# make sure, that if the corresponding files exist, they are emptied\n",
        "if os.path.isfile( path_to_training_file ):\n",
        "\n",
        "  open( path_to_training_file, \"w\" ).close()\n",
        "\n",
        "if os.path.isfile( path_to_test_file ):\n",
        "\n",
        "  open( path_to_test_file, \"w\" ).close()\n",
        "\n",
        "\n",
        "# open empty files\n",
        "training_file = open( path_to_training_file, \"a\", encoding = \"utf-8\" )\n",
        "    \n",
        "test_file = open( path_to_test_file, \"a\", encoding = \"utf-8\" )\n",
        "\n",
        "remove = [\"\", \" \", \"\\n\"]\n",
        "\n",
        "for item in training_data_as_bilou:\n",
        "\n",
        "  # TODO remove all tokens like \"\", \" \", \"\\n\" by ignoring them\n",
        "  # for all other tokens do the following:\n",
        "  # create a string s: s = token + \" \" + label + \"\\n\"\n",
        "  # if the label is \"-\", then write s = token + \" O\\n\"\n",
        "  #\n",
        "  # write this newly created string to file\n",
        "  # if this newly created string contains \".\", then also write a \n",
        "  # newline to file that only contains \"\\n\"\n",
        "  #\n",
        "  # Using this scheme, each line in the resulting files corresponds either to an empty line or a token.\n",
        "  # Flair assembles a block of nonempty lines into a sentence. Therefore, the empty line\n",
        "  # is a signal for Flair that the current sentence is finished. Therefore, we extracted\n",
        "  # the whitespaces above.\n",
        "  removed_items = item[~item[\"Tokens\"].isin(remove)].copy()\n",
        "\n",
        "  for token, label in zip(removed_items[\"Tokens\"], removed_items[\"True\"]):\n",
        "    s = \"\"\n",
        "\n",
        "    if label == \"-\":\n",
        "      s = token + \" O\\n\"\n",
        "    else:\n",
        "      s = token + \" \" + label + \"\\n\"\n",
        "\n",
        "    training_file.write(s)\n",
        "\n",
        "    if \".\" in s:\n",
        "      training_file.write(\"\\n\")\n",
        "\n",
        "\n",
        "for item in test_data_as_bilou:\n",
        "    \n",
        "  # TODO the same as above.\n",
        "  removed_items = item[~item[\"Tokens\"].isin(remove)].copy()\n",
        "\n",
        "  for token, label in zip(removed_items[\"Tokens\"], removed_items[\"True\"]):\n",
        "    s = \"\"\n",
        "\n",
        "    if label == \"-\":\n",
        "      s = token + \" O\\n\"\n",
        "    else:\n",
        "      s = token + \" \" + label + \"\\n\"\n",
        "\n",
        "    test_file.write(s)\n",
        "\n",
        "    if \".\" in s:\n",
        "      test_file.write(\"\\n\")\n",
        "\n",
        "\n",
        "training_file.close()\n",
        "\n",
        "test_file.close()"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V5trnr4gXa9"
      },
      "source": [
        "Start Flair"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "630pwinWhXWo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1facb69f-ce7c-4e8c-b4d2-87e0d52b4f6f"
      },
      "source": [
        "pip install flair"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting flair\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/3a/1b46a0220d6176b22bcb9336619d1731301bc2c75fa926a9ef953e6e4d58/flair-0.8.0.post1-py3-none-any.whl (284kB)\n",
            "\r\u001b[K     |█▏                              | 10kB 22.1MB/s eta 0:00:01\r\u001b[K     |██▎                             | 20kB 29.3MB/s eta 0:00:01\r\u001b[K     |███▌                            | 30kB 21.2MB/s eta 0:00:01\r\u001b[K     |████▋                           | 40kB 16.3MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 51kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████                         | 61kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 71kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 81kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 92kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 102kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 112kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 122kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 133kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 143kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 153kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 163kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 174kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 184kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 194kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 204kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 215kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 225kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 235kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 245kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 256kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 266kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 276kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 286kB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.41.1)\n",
            "Collecting gdown==3.12.2\n",
            "  Downloading https://files.pythonhosted.org/packages/50/21/92c3cfe56f5c0647145c4b0083d0733dd4890a057eb100a8eeddf949ffe9/gdown-3.12.2.tar.gz\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting mpld3==0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n",
            "\u001b[K     |████████████████████████████████| 798kB 41.1MB/s \n",
            "\u001b[?25hCollecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/72/a3add0e4eec4eb9e2569554f7c70f4a3c27712f40e3284d483e88094cc0e/langdetect-1.0.9.tar.gz (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 34.5MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.95\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 43.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from flair) (0.8.9)\n",
            "Requirement already satisfied: numpy<1.20.0 in /usr/local/lib/python3.7/dist-packages (from flair) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from flair) (2.8.1)\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/b5/5da463f9c7823e0e575e9908d004e2af4b36efa8d02d3d6dad57094fcb11/ftfy-6.0.1.tar.gz (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from flair) (4.2.6)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from flair) (2019.12.20)\n",
            "Collecting deprecated>=1.2.4\n",
            "  Downloading https://files.pythonhosted.org/packages/fb/73/994edfcba74443146c84b91921fcc269374354118d4f452fb0c54c1cbb12/Deprecated-1.2.12-py2.py3-none-any.whl\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from flair) (0.22.2.post1)\n",
            "Collecting torch<=1.7.1,>=1.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/5d/095ddddc91c8a769a68c791c019c5793f9c4456a688ddd235d6670924ecb/torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8MB 24kB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading https://files.pythonhosted.org/packages/32/a1/7c5261396da23ec364e296a4fb8a1cd6a5a2ff457215c6447038f18c0309/huggingface_hub-0.0.9-py3-none-any.whl\n",
            "Requirement already satisfied: gensim<=3.8.3,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from flair) (3.6.0)\n",
            "Collecting sqlitedict>=1.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/2d/b1d99e9ad157dd7de9cd0d36a8a5876b13b55e4b75f7498bc96035fb4e96/sqlitedict-1.7.0.tar.gz\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from flair) (3.2.2)\n",
            "Collecting bpemb>=0.3.2\n",
            "  Downloading https://files.pythonhosted.org/packages/f2/6f/9191b85109772636a8f8accb122900c34db26c091d2793218aa94954524c/bpemb-0.3.3-py3-none-any.whl\n",
            "Collecting transformers>=4.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 31.4MB/s \n",
            "\u001b[?25hCollecting konoha<5.0.0,>=4.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/02/be/4dd30d56a0a19619deb9bf41ba8202709fa83b1b301b876572cd6dc38117/konoha-4.6.4-py3-none-any.whl\n",
            "Collecting janome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/63/98858cbead27df7536c7e300c169da0999e9704d02220dc6700b804eeff0/Janome-0.4.1-py2.py3-none-any.whl (19.7MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from flair) (0.1.2)\n",
            "Collecting segtok>=1.5.7\n",
            "  Downloading https://files.pythonhosted.org/packages/41/08/582dab5f4b1d5ca23bc6927b4bb977c8ff7f3a87a3b98844ef833e2f5623/segtok-1.5.10.tar.gz\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (1.15.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (3.0.12)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->flair) (0.2.5)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.4->flair) (1.12.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair) (1.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<=1.7.1,>=1.5.0->flair) (3.7.4.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->flair) (4.0.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim<=3.8.3,>=3.4.0->flair) (5.0.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (2.4.7)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 31.6MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 28.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (20.9)\n",
            "Collecting overrides<4.0.0,>=3.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/b1/10f69c00947518e6676bbd43e739733048de64b8dd998e9c2d5a71f44c5d/overrides-3.1.0.tar.gz\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (3.11.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (2.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown==3.12.2->flair) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown==3.12.2->flair) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown==3.12.2->flair) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown==3.12.2->flair) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown==3.12.2->flair) (1.7.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->huggingface-hub->flair) (3.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.0.0->flair) (8.0.0)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->hyperopt>=0.1.1->flair) (4.4.2)\n",
            "Building wheels for collected packages: gdown\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-3.12.2-cp37-none-any.whl size=9693 sha256=fa26ceaf779cc33a87a0834633759473d15ea868103b242e022cd1083e81f67b\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/d0/d7/d9983facc6f2775411803e0e2d30ebf98efbf2fc6e57701e09\n",
            "Successfully built gdown\n",
            "Building wheels for collected packages: mpld3, langdetect, ftfy, sqlitedict, segtok, overrides\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-cp37-none-any.whl size=116679 sha256=0d1611fe297b590434d4db17ab43e206684bd81cc5b00ac6089c2f808b8ff579\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-cp37-none-any.whl size=993223 sha256=0ea020943e94ad2891ef43cb85df12c89ffc9bfeb52ae52dea7942fc0313a0a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/7e/18/13/038c34057808931c7ddc6c92d3aa015cf1a498df5a70268996\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.1-cp37-none-any.whl size=41573 sha256=714c5bce43face610027da49ec60b44c5992d26fac5af42f584785df8c67c0a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/73/c7/9056e14b04919e5c262fe80b54133b1a88d73683d05d7ac65c\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-cp37-none-any.whl size=14376 sha256=4106e05b50c777321e66481eb9be0c6276b21e658426dfdb2da26328c19820f8\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/c6/4f/2c64a43f041415eb8b8740bd80e15e92f0d46c5e464d8e4b9b\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segtok: filename=segtok-1.5.10-cp37-none-any.whl size=25019 sha256=18c99e553391fa6da45055c9a3390db59f0b7661610cb819bceedbd9d9cb7fc7\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/39/f6/9ca1c5cabde964d728023b5751c3a206a5c8cc40252321fb6b\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-cp37-none-any.whl size=10174 sha256=f1439a619ddea68a77b7d8c73a3db60f4ae60c5f76f3283f8122d435febe09b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/24/13/6ef8600e6f147c95e595f1289a86a3cc82ed65df57582c65a9\n",
            "Successfully built mpld3 langdetect ftfy sqlitedict segtok overrides\n",
            "\u001b[31mERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: transformers 4.6.1 has requirement huggingface-hub==0.0.8, but you'll have huggingface-hub 0.0.9 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: konoha 4.6.4 has requirement importlib-metadata<4.0.0,>=3.7.0, but you'll have importlib-metadata 4.0.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: konoha 4.6.4 has requirement requests<3.0.0,>=2.25.1, but you'll have requests 2.23.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gdown, mpld3, langdetect, sentencepiece, ftfy, deprecated, torch, huggingface-hub, sqlitedict, bpemb, sacremoses, tokenizers, transformers, overrides, konoha, janome, segtok, flair\n",
            "  Found existing installation: gdown 3.6.4\n",
            "    Uninstalling gdown-3.6.4:\n",
            "      Successfully uninstalled gdown-3.6.4\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "Successfully installed bpemb-0.3.3 deprecated-1.2.12 flair-0.8.0.post1 ftfy-6.0.1 gdown-3.12.2 huggingface-hub-0.0.9 janome-0.4.1 konoha-4.6.4 langdetect-1.0.9 mpld3-0.3 overrides-3.1.0 sacremoses-0.0.45 segtok-1.5.10 sentencepiece-0.1.95 sqlitedict-1.7.0 tokenizers-0.10.2 torch-1.7.1 transformers-4.6.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKoPUsQGgaEE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "381172a9-121a-4b16-eed4-e31bfb2a16c7"
      },
      "source": [
        "from flair.data import Corpus\n",
        "from flair.data_fetcher import NLPTaskDataFetcher\n",
        "\n",
        "\n",
        "# your training file name\n",
        "data_folder = os.getcwd() \n",
        "\n",
        "train_file = \"training_data.csv\"\n",
        "\n",
        "# your training file name\n",
        "test_file = \"test_data.csv\"\n",
        "\n",
        "# when we wrote the data files, each row was either empty to signal the end\n",
        "# of a sentence to Flair, or the line contained a token, a white space and a label.\n",
        "# In the next line, we assign, that the token is the \"text\", and that the label is \n",
        "# \"ner\" label\n",
        "columns =  {0: 'text', 1: 'ner'}\n",
        "\n",
        "## Now load our csv into flair corpus\n",
        "corpus = NLPTaskDataFetcher.load_column_corpus(data_folder,column_format=columns,\n",
        "                                               train_file=train_file,\n",
        "                                               test_file=test_file)\n",
        "print(corpus)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-22 10:54:43,090 Reading data from /content/gdrive/My Drive/flair\n",
            "2021-05-22 10:54:43,098 Train: /content/gdrive/My Drive/flair/training_data.csv\n",
            "2021-05-22 10:54:43,101 Dev: None\n",
            "2021-05-22 10:54:43,102 Test: /content/gdrive/My Drive/flair/test_data.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated function (or staticmethod) load_column_corpus. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Corpus: 6290 train + 699 dev + 3322 test sentences\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}